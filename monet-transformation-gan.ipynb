{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30748,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, LeakyReLU, ReLU, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanAbsoluteError\nimport numpy as np\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport zipfile\nimport PIL\nimport shutil\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-23T03:22:18.971097Z","iopub.execute_input":"2024-08-23T03:22:18.971971Z","iopub.status.idle":"2024-08-23T03:22:18.979429Z","shell.execute_reply.started":"2024-08-23T03:22:18.971937Z","shell.execute_reply":"2024-08-23T03:22:18.978527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Brief Description: \nThe project involves the transformation of photos into the style of Monet paintings using a Generative Adversarial Network (GAN). The dataset consists of two folders: one containing Monet paintings (monet_jpg/) and another containing photos to be transformed (photo_jpg/). The goal is to generate Monet-style images from the photos and save them in a ZIP file for submission.","metadata":{}},{"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(\n        Conv2D(filters, size, strides=2, padding='same',\n               kernel_initializer=initializer, use_bias=False))\n\n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(LeakyReLU())\n\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(\n        Conv2DTranspose(filters, size, strides=2,\n                        padding='same',\n                        kernel_initializer=initializer,\n                        use_bias=False))\n\n    result.add(tf.keras.layers.BatchNormalization())\n\n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n\n    result.add(ReLU())\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-23T19:31:56.163126Z","iopub.execute_input":"2024-08-23T19:31:56.163479Z","iopub.status.idle":"2024-08-23T19:31:56.180681Z","shell.execute_reply.started":"2024-08-23T19:31:56.163448Z","shell.execute_reply":"2024-08-23T19:31:56.179798Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def Generator():\n    inputs = Input(shape=[256, 256, 3])  # Accept input with shape (256, 256, 3)\n\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n        downsample(512, 4),\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4, apply_dropout=True),\n        upsample(512, 4),\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4),\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = Conv2DTranspose(3, 4,\n                           strides=2,\n                           padding='same',\n                           kernel_initializer=initializer,\n                           activation='tanh')\n\n    x = inputs\n\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = Concatenate()([x, skip])\n\n    x = last(x)\n\n    return Model(inputs=inputs, outputs=x)\n\ndef Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    inp = Input(shape=[256, 256, 3], name='input_image')  # Accept input with shape (256, 256, 3)\n    tar = Input(shape=[256, 256, 3], name='target_image')\n\n    x = Concatenate()([inp, tar])\n\n    down1 = downsample(64, 4, False)(x)\n    down2 = downsample(128, 4)(down1)\n    down3 = downsample(256, 4)(down2)\n\n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)\n    conv = Conv2D(512, 4, strides=1,\n                  kernel_initializer=initializer,\n                  use_bias=False)(zero_pad1)\n\n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n    leaky_relu = LeakyReLU()(batchnorm1)\n\n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n\n    last = Conv2D(1, 4, strides=1,\n                  kernel_initializer=initializer)(zero_pad2)\n\n    return Model(inputs=[inp, tar], outputs=last)\n\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, generated_output):\n    real_loss = loss_obj(tf.ones_like(real_output), real_output)\n    generated_loss = loss_obj(tf.zeros_like(generated_output), generated_output)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss * 0.5\n\ndef generator_loss(generated_output):\n    return loss_obj(tf.ones_like(generated_output), generated_output)\n\ndef cycle_consistency_loss(real_image, cycled_image):\n    loss = MeanAbsoluteError()\n    return loss(real_image, cycled_image) * 10.0\n\ndef identity_loss(real_image, same_image):\n    loss = MeanAbsoluteError()\n    return loss(real_image, same_image) * 5.0\n\ngenerator_g = Generator()\ngenerator_f = Generator()\n\ndiscriminator_x = Discriminator()\ndiscriminator_y = Discriminator()\n\ngenerator_g_optimizer = Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:22:18.996788Z","iopub.execute_input":"2024-08-23T03:22:18.997082Z","iopub.status.idle":"2024-08-23T03:22:19.743372Z","shell.execute_reply.started":"2024-08-23T03:22:18.997050Z","shell.execute_reply":"2024-08-23T03:22:19.742532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n    image = tf.image.resize(image, [256, 256])\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image  # Return image with shape (256, 256, 3)\n\ndef load_data(path):\n    data = []\n    for img in glob(os.path.join(path, \"*.jpg\")):\n        image = load_image(img)\n        data.append(image)\n    return np.array(data)  # Combine into one array with shape (N, 256, 256, 3)\n\n# Load Monet images from the 'train' directory\nmonet_images = load_data('/kaggle/input/gan-getting-started/monet_jpg')\n\n# Load images to be transformed from the 'test' directory\ntarget_images = load_data('/kaggle/input/gan-getting-started/photo_jpg')\n\nEPOCHS = 3\n\n@tf.function\ndef train_step(real_x, real_y):\n    # Ensure batch dimension\n    real_x = tf.expand_dims(real_x, axis=0) if len(real_x.shape) == 3 else real_x\n    real_y = tf.expand_dims(real_y, axis=0) if len(real_y.shape) == 3 else real_y\n    \n    with tf.GradientTape(persistent=True) as tape:\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n\n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n\n        # The discriminator expects both the real/generated image and the corresponding target image\n        disc_real_x = discriminator_x([real_x, real_y], training=True)\n        disc_real_y = discriminator_y([real_y, real_x], training=True)\n\n        disc_fake_x = discriminator_x([fake_x, real_y], training=True)\n        disc_fake_y = discriminator_y([fake_y, real_x], training=True)\n\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n\n        total_cycle_loss = cycle_consistency_loss(real_x, cycled_x) + cycle_consistency_loss(real_y, cycled_y)\n\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n    generator_g_gradients = tape.gradient(total_gen_g_loss,\n                                          generator_g.trainable_variables)\n    generator_f_gradients = tape.gradient(total_gen_f_loss,\n                                          generator_f.trainable_variables)\n\n    discriminator_x_gradients = tape.gradient(disc_x_loss,\n                                              discriminator_x.trainable_variables)\n    discriminator_y_gradients = tape.gradient(disc_y_loss,\n                                              discriminator_y.trainable_variables)\n\n    generator_g_optimizer.apply_gradients(zip(generator_g_gradients,\n                                              generator_g.trainable_variables))\n\n    generator_f_optimizer.apply_gradients(zip(generator_f_gradients,\n                                              generator_f.trainable_variables))\n\n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                  discriminator_x.trainable_variables))\n\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                  discriminator_y.trainable_variables))\n\ndef train(dataset_x, dataset_y, epochs):\n    for epoch in range(epochs):\n        for image_x, image_y in zip(dataset_x, dataset_y):\n            train_step(image_x, image_y)\n        print(f'Epoch {epoch+1}/{epochs} completed.')\n\ntrain(monet_images, target_images, EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:22:19.745457Z","iopub.execute_input":"2024-08-23T03:22:19.745885Z","iopub.status.idle":"2024-08-23T03:27:09.424524Z","shell.execute_reply.started":"2024-08-23T03:22:19.745852Z","shell.execute_reply":"2024-08-23T03:27:09.423531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EDA\nprint(f\"Number of Monet images: {len(monet_images)}\")\nprint(f\"Number of photo images: {len(photo_images)}\")\n\ndef display_images(images, path, title, n=5):\n    plt.figure(figsize=(15, 5))\n    for i in range(n):\n        img = Image.open(os.path.join(path, images[i]))\n        plt.subplot(1, n, i+1)\n        plt.imshow(img)\n        plt.title(f\"{title} {i+1}\")\n        plt.axis('off')\n    plt.show()\n\n# Display sample Monet images\ndisplay_images(monet_images, monet_path, \"Monet Image\")\n\n# Display sample Photo images\ndisplay_images(photo_images, photo_path, \"Photo Image\")\n\ndef check_image_dimensions(images, path):\n    dimensions = []\n    for img_name in images:\n        img = Image.open(os.path.join(path, img_name))\n        dimensions.append(img.size)\n    return dimensions\n\n# Check dimensions of Monet images\nmonet_dimensions = check_image_dimensions(monet_images, monet_path)\nprint(f\"Monet Image Dimensions: {set(monet_dimensions)}\")\n\n# Check dimensions of Photo images\nphoto_dimensions = check_image_dimensions(photo_images, photo_path)\nprint(f\"Photo Image Dimensions: {set(photo_dimensions)}\")\n\n#EDA Summary\n#Number of Images: Both the Monet and photo datasets contain a substantial number of images.\n#Image Dimensions: It's important to confirm that all images have the same dimensions to ensure compatibility with the model.\n#Visual Inspection: Sample images from both datasets give an insight into the visual differences the model needs to learn to transform.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Analysis and Summary\n#Generators:\n\n#generator_g: Transforms photos into Monet-style images.\n#generator_f: Transforms Monet-style images back into photos.\n#Discriminators:\n\n#discriminator_x: Differentiates between real photos and generated photos.\n#discriminator_y: Differentiates between real Monet images and generated Monet-style images.\n\n#Generator Loss: Encourages the generator to produce realistic images that can fool the discriminator.\n#Discriminator Loss: Ensures the discriminator accurately distinguishes between real and fake images.\n#Cycle Consistency Loss: Ensures that after translating an image to the target domain and back, the resulting image is close to the original.\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model Performance and Results\ndef generate_and_zip_images(model, photo_images_paths, output_dir, zip_name):\n    # Create a directory to save the generated images\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Generate images and save them to the directory\n    i = 1\n    for img_path in photo_images_paths:\n        img = load_image(img_path)  # Load the image\n        img = tf.expand_dims(img, 0)  # Add batch dimension\n\n        # Generate the Monet-style image\n        prediction = model(img, training=False)[0].numpy()\n\n        # Convert the generated image from [-1, 1] to [0, 255]\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n\n        # Save the image as a JPEG file\n        img_save_path = os.path.join(output_dir, f\"{i}.jpg\")\n        im = PIL.Image.fromarray(prediction)\n        im.save(img_save_path)\n        print(f\"Saved image {i} at {img_save_path}\")\n        i += 1\n\n    # Check if images were saved\n    saved_images = os.listdir(output_dir)\n    if saved_images:\n        print(f\"Successfully saved {len(saved_images)} images.\")\n        print(\"Sample images:\", saved_images[:5])  # Show first 5 images\n    else:\n        print(\"No images were saved.\")\n\n    # Create a ZIP file with all the generated images\n    shutil.make_archive(zip_name, 'zip', output_dir)\n    print(f\"All images have been saved and zipped into {zip_name}.zip\")\n\n# Direct path to the test images for generation\nphoto_images_paths = glob('/kaggle/input/gan-getting-started/photo_jpg/*.jpg')\noutput_dir = \"images\"\nzip_name = \"images\"\n\n# Generate images and save them into a zip file\ngenerate_and_zip_images(generator_g, photo_images_paths, output_dir, zip_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-23T03:27:09.426095Z","iopub.execute_input":"2024-08-23T03:27:09.426479Z","iopub.status.idle":"2024-08-23T03:33:25.245813Z","shell.execute_reply.started":"2024-08-23T03:27:09.426446Z","shell.execute_reply":"2024-08-23T03:33:25.244824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Conclusion\n#In this project, I successfully built and trained a CycleGAN model to transform photos into Monet-style images. \n#By exploring the dataset, I ensured consistency in image dimensions and visualized the data to understand its structure better. \n#I examined the model architecture and analyzed the training dynamics, focusing on the importance of generator, discriminator, and cycle consistency losses in achieving the desired style transfer.\n\n#The generated images were then assessed, and I was pleased to see that the model convincingly applied Monet's style to the photos. \n#While the results were promising, I recognize that there is room for improvement through hyperparameter tuning and architectural enhancements. Overall, this project demonstrated the potential of GANs for artistic style transfer and provided a solid foundation for future exploration in this area.","metadata":{},"execution_count":null,"outputs":[]}]}